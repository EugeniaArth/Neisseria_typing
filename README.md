# Neisseria_typing

Разработка новых схем молекулярного типирования N. gonorrhoeae, для улучшения эпидемиологического мониторинга, отслеживание распространения антибиотико-резистентных штаммов, изучение генетического разнообразия бактерии и создание основы для разработки эффективных методов диагностики, лечения и профилактики гонококковой инфекции.
 
N. gonorrhoeae - культивируемый микроорганизм. В настоящее время ведется типирование по двум основным системам - гипервариабельные гены, входящие в NG-MAST и  MLST включает гены домашнего хозяйства. По первой слишком много генотипов, вторая наоборот слишком консервативная. Позволяет отслеживать распространение и выявить связь с антибиотикорезистентностью

Задача:
Разработать новую систему эпидемиологического контроля за распространением гонококковой инфекции , которая позволит классифицировать штаммы по происхождению, выявить связь с антибиотикорезистентностью, но не будет слишком консервативна/слишком вариабельная, а будет давать меньшее количество типов чтобы легче работать

Референсный геном штамма FA19 https://www.ncbi.nlm.nih.gov/assembly/GCF_001047225.1 


Этапы. 

1) Подготовка данных - fastq для изолятов из базы данных PubMLST. 
2) Поиск снипов по протоколу GATK. (Возможно не понадобится). Если есть возможность совместить эти данные по всем заменам во всех генах с филогенетикой, то идеально
4) Дополнительно (или вместо GATK) анализ  резистентности по алгоритму CARD для .fasta 
5) Аннотирование .fasta при помощи bakta https://github.com/oschwengers/bakta
6) Поиск ортологов (всех ортологов скорее всего, а не только коровых?)
7) Построение идеального модельного дерева ИЛИ формулировка критериев кластеризации
8) Выравнивание и построение филогенетических деревьев с учетом п.5



Общая структура папок выклядит как-то так:

**Подготовка данных**
Изоляты скачиваются из БД PubMLST, она содержит .fasta и .fas файлы (Но не SRA).  Названия сэмплов и стран, которые были депонированы на диске, анализированы скриптом get_samples_names.py (так как мне достались только папке с файлами, без описания, то пришлось сначала собрать все названия файлов и папок, потом выгрузить данные по ID, которое соответствовало названиям сэмплов всю информацию по ним из БД, в том числе SRA). Таблица analyzed_data.xlsx содержит основные сведения по загруженным файлам

**Поиск снипов по протоколу GATK**
Проводилось в несколько этапов:

1) Скачивание fastq используя данные Run Accession для каждого изолята. Конвертированы для удобства в tab-delimited файл for_fasterq-dump.txt. Скрипт для скачивания fasterq-dump.sh
2) Анализ качества полученных исходных данных и тримминг при необходимости. Анализ качества проводим fastqc и multiqc скриптом quality_check.sh. Затем - полученные multiqc файлы скриптом multiqc_analysis.py анализируем для контроля, какие образцы провалили проверку. Результат сохраняется в виде failed_samples.txt (те же скрипты можно использовать для контроля после тримминга, только выбрать другую директорию). Тримминг проводится скриптом trimming.py (обязательно указать путь для файла с адаптерами). После тримминга и повторной проверки, для хороших образцов и тех, которые пройдут повторную проверку можно приступать к картированию
3) Картирование на референсный геном (FA19) проводится bwa mem с последующем конвертированием .sam в .bam, сортировкой и индексированием – скрипт mapping.sh
5) Поиск снипов по протоколу GATK и аннотация выявленных замен. Используем скрипт calling.sh.  Аннотация вариантов проводится при помощи Annovar (https://annovar.openbioinformatics.org/en/latest/user-guide/filter/#overview) - для этого ему нужна база данных dbSNP. Так как для N.gonorrhoeae ее не существует, ее нужно создать по инструкции. Готовая база данных для данного проекта в архиве annovar_db.zip, после разархивирования нужно указать путь к ней в скрипте (Также, если java -jar picard.jar AddOrReplaceReadGroups и подобные не работают, то нужен полный путь к picard.jar).
6) Анализ данных.
   В результате работы пайплайна в папке calling по странам в каждой будут финальные данные в виде подпапок bam, vcf и vcf_annotated. В последней нас интересуют файлы .snp.refGene.exonic_variant_function. Скрипт parsing_GATK_output.py экстрагирует данные из этих файлов и создает файлы {sample}_processed_data.csv с результатами вида "Start", "End", "Locus", "Gene name", "Gene biotype", "Protein name", "Protein ID", "Type of SNP", "Position", "SNP", "AA change" с найденными снипами для каждого файла и для каждой страны в отдельных папках в /GATK4/analysis. Наконец, скрипт analysis_summary.py анализирует {sample}_processed_data.csv и формирует саммари для каждой страны и итоговое для всех сэмплов. Итоговый файл в виде архива combined_summary.csv.zip есть в репозитории (прим.: содержит более 500 000 строк).

**Анализ  резистентности по алгоритму CARD**
Устанавливаем RGI и проводим поиск скриптом resistance_search.sh (запускаем через docker) – процесс занимает около 8-9 часов с использованием 32 CPU и 128 Мб памяти

**Аннотирование .fasta при помощи bakta**   
Так как доя изолятов в PubMLST нет стандарных аннотаций в виде gff или gbff, нужно аннотировать. Описание тут https://github.com/oschwengers/bakta. Скрипт bakta.sh – в результате имеем файл с отдельными белкавыми последовательностями для каждого сэмпла и аннотацию, которые можно использовать далее.

**Поиск ортологов**

Используем https://gitlab.com/paulklemm_PHD/proteinortho, скрипт proteinortho.sh. Для поиска ортологов он использует diamond blast помимо других программ, так что для 400+ образцов занимает несколько часов. 

**Выравнивание и построение филогенетического дерева**

В моей голове это должно выглядеть как итеративный перебор различных комбинаций генов  в количестве 5-8  шт - при этом берем 1 набор ортологов, выравниваем,  дерево. Потом делаем прибавление последовательностей из других наборов ортологов к 1 набору стык в стык и выравнивание, в итоге это выглядит так:
>сэмпл1_ген1_ген2_ген3
последовательность1+последовательность2+последовательность3
>сэмпл2_ортологичеый ген1_ ... и тд.

Каждая итерация будет сравниваться с идеальным деревом или оцениваться както еще, главное чтобы автоматически, пока не знаю как

 Однако я понимаю что рандомно извлекать случайные ортологи долго, печально и не поможет.  Потому что выбирать только самые вариабельные гены возможно не поможет, так как они дадут слишком много кластеров, а надо чтобы 1) какие-то гены отвечали за кластеризацию с учетом источника, страны 2) другие за резистентность. И сделав панель на нескольок генов мы бы решили все вопросы, откуда штамм, местный иом завозной + насколько он устойчив
